### LLM

- [GitHub - mlrun/demo-llm-tuning: Demo MLRun project for LLM Tuning and serving pipelines](https://github.com/mlrun/demo-llm-tuning)
- [模型列表_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/getting-started/models?spm=a2c4g.11186623.help-menu-2400256.d_0_2.4d7220efgSTxpX&scm=20140722.H_2840914._.OR_help-T_cn~zh-V_1#ced16cb6cdfsy)
- [在控制台使用模型调优_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/user-guide/using-fine-tuning-on-console?spm=a2c4g.11186623.help-menu-2400256.d_1_3_0.42a635254ifadK&scm=20140722.H_2587462._.OR_help-T_cn~zh-V_1#f161782782o0n)
- [模型推理、模型训练、模型部署费用说明_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/billing-for-model-studio?spm=a2c4g.11186623.0.0.3cba5991yReIGZ#7a3514acc706i)
- [Code completion | IntelliJ IDEA Documentation](https://www.jetbrains.com/help/idea/auto-completing-code.html#basic_completion)
- [GitHub - JetBrains/intellij-sdk-docs: IntelliJ SDK Platform Documentation](https://github.com/JetBrains/intellij-sdk-docs)
- [GitHub - continuedev/continue: Continue is the leading open-source AI code assistant. You can connect any models and any context to build custom autocomplete and chat experiences inside VS Code and JetBrains](https://github.com/continuedev/continue)
- [copilot-explorer | Hacky repo to see what the Copilot extension sends to the server](https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html)
- [How it works | Continue](https://docs.continue.dev/autocomplete/how-it-works)
- [GitHub - JetBrains/intellij-community: IntelliJ IDEA Community Edition & IntelliJ Platform](https://github.com/jetbrains/intellij-community)
- [mlrun - Using LLMs to process unstructured data](https://docs.mlrun.org/en/v1.6.4/genai/data-mgmt/unstructured-data.html#turning-unstructured-data-into-structured-data)
- [aliyun - RAG效果优化_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/use-cases/rag-optimization?spm=a2c4g.11186623.help-menu-2400256.d_2_12.4f01716edKjj8o&scm=20140722.H_2859102._.OR_help-T_cn~zh-V_1)
- [GitHub - jlonge4/local_llama: This repo is to showcase how you can run a model locally and offline, free of OpenAI dependencies.](https://github.com/jlonge4/local_llama)
- [Ollama](https://ollama.com/)
- [GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.](https://github.com/ollama/ollama)
- [GitHub - Mintplex-Labs/anything-llm: The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, and more.](https://github.com/Mintplex-Labs/anything-llm)
- [localai.io - Overview | LocalAI documentation](https://localai.io/)
- [ollama - deepseek-r1](https://ollama.com/library/deepseek-r1)
- [阿里云 - RAG 知识库](https://help.aliyun.com/zh/model-studio/user-guide/rag-knowledge-base?spm=a2c4g.11186623.help-menu-2400256.d_1_8_2_0_0.4d666373lBKTyJ)
- [Using LLMs to process unstructured data](https://docs.mlrun.org/en/v1.6.4/genai/data-mgmt/unstructured-data.html#turning-unstructured-data-into-structured-data)
- [MLRun · GitHub](https://github.com/mlrun)
- [GitHub - mlrun/demo-llm-tuning: Demo MLRun project for LLM Tuning and serving pipelines](https://github.com/mlrun/demo-llm-tuning)
- [GitHub - facebookincubator/Glean: System for collecting, deriving and working with facts about source code.](https://github.com/facebookincubator/Glean)
- [Introduction | Glean](https://glean.software/docs/introduction/)
- [GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.](https://github.com/ollama/ollama?tab=readme-ov-file)
- [Fine-Tuning With Ollama Techniques | Restackio](https://www.restack.io/p/fine-tuning-answer-ollama-techniques-cat-ai)
- [deepseek-r1:1.5b](https://ollama.com/library/deepseek-r1:1.5b)
https://dev.to/mohsin_rashid_13537f11a91/rag-with-ollama-1049, HTTP Error 403:
- [ollama/docs/modelfile.md at main · ollama/ollama · GitHub](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#build-from-existing-model)
- [GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)
- [GitHub - langgenius/dify: Dify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.](https://github.com/langgenius/dify)
- [How to use JSON references ($refs)](https://redocly.com/learn/openapi/ref-guide)
- [Qwen/Qwen2.5-32B · Hugging Face](https://huggingface.co/Qwen/Qwen2.5-32B)
- [GitHub - sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models.](https://github.com/sgl-project/sglang)
- [GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)
- [GitHub - facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors.](https://github.com/facebookresearch/faiss?tab=readme-ov-file)
https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/, HTTP Error 403:
- [GitHub - Lightning-AI/litgpt: 20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.](https://github.com/Lightning-AI/litgpt)
- [GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1 & Reasoning LLMs 2x faster with 70% less memory! 🦥](https://github.com/unslothai/unsloth?tab=readme-ov-file)
https://unsloth.ai/blog/deepseek-r1, HTTP Error 403:
https://www.datacamp.com/tutorial/fine-tuning-large-language-models, HTTP Error 403:
- [Implementing RAG in Refact.ai AI Coding Assistant | by Refact.ai | Medium](https://medium.com/@refact_ai/implementing-rag-in-refact-ai-ai-coding-assistant-79e98b359a83)
- [generative-ai/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb at main · GoogleCloudPlatform/generative-ai · GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb)
- [RAG For a Codebase with 10k Repos](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/)
- [Chunking 2M+ files a day for Code Search using Syntax Trees](https://docs.sweep.dev/blogs/chunking-2m-files)
- [sweep/sweepai/utils/utils.py at b267b613d4c706eaf959fe6789f11e9a856521d1 · sweepai/sweep · GitHub](https://github.com/sweepai/sweep/blob/b267b613d4c706eaf959fe6789f11e9a856521d1/sweepai/utils/utils.py#L48-L126)
- [GitHub - run-llama/llama_index: LlamaIndex is the leading framework for building LLM-powered agents over your data.](https://github.com/run-llama/llama_index)
- [LlamaIndex - LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [GGUF](https://huggingface.co/docs/hub/en/gguf)
- [Continue - Codestral, Claude, and more - Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
- [Explore LLM using LangChain and HuggingFace | kekw](https://curtisnewbie.github.io/learning/2024/06/24/explore-llm-using-langchain-and-huggingface.html)
- [GitHub - qiyu-kefu/message_interface: 七鱼消息接口以及简单微信公众号开发模式示例demo](https://github.com/qiyu-kefu/message_interface)
- [deepseek-r1 32B需要什么配置 - CSDN文库](https://wenku.csdn.net/answer/7nz5wit3zf)
- [如何本地部署DeepSeek_云服务器 ECS(ECS)-阿里云帮助中心](https://help.aliyun.com/zh/ecs/use-cases/deploy-the-deepseek-r1-distill-model-on-a-gpu-accelerated-instance)
- [NVIDIA的GA107、A10、V100、H100、H200和消费级显卡的区别英伟达的GA107、A10和V100是不 - 掘金](https://juejin.cn/post/7371273982200004635)
- [NVIDIA A10 与 A100 GPU 对比分析：用于LLM 和Stable Diffusion推理](https://www.jaeaiot.com/news/detail/295.html)
- [Using VLLM with Langchain for RAG purposes · langchain-ai/langchain · Discussion #22947 · GitHub](https://github.com/langchain-ai/langchain/discussions/22947)
- [FAISS Vector Database for Production LLM Applications | by Wamiq Raza | Python’s Gurus | Medium](https://medium.com/pythons-gurus/faiss-vector-database-for-production-llm-applications-90273c78fcbf)
- [Faiss | 🦜️🔗 LangChain](https://python.langchain.com/docs/integrations/vectorstores/faiss/)
- [Welcome to Faiss Documentation — Faiss  documentation](https://faiss.ai/index.html)
- [faiss/INSTALL.md at main · facebookresearch/faiss · GitHub](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md)
- [GitHub - deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file)
- [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B#usage-recommendations)
- [deepseek-ai (DeepSeek)](https://huggingface.co/deepseek-ai)
- [Models - Hugging Face](https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)
- [vLLM | 🦜️🔗 LangChain](https://python.langchain.com/docs/integrations/llms/vllm/)
- [llmc/docs/zh_cn/source/backend/vllm.md at main · ModelTC/llmc · GitHub](https://github.com/ModelTC/llmc/blob/main/docs/zh_cn/source/backend/vllm.md)
- [How to use Streaming in LangChain and Streamlit · Alejandro AO](https://alejandro-ao.com/how-to-use-streaming-in-langchain-and-streamlit/)
- [How to stream runnables | 🦜️🔗 LangChain](https://python.langchain.com/docs/how_to/streaming/)
- [Help: 在ollama上运行时, 怎么才能让它不返回<think>标签 ?  When running on ollama, how can I make it not return the < think > tag? · Issue #23 · deepseek-ai/DeepSeek-R1 · GitHub](https://github.com/deepseek-ai/DeepSeek-R1/issues/23)
- [Reasoning Outputs — vLLM](https://docs.vllm.ai/en/latest/features/reasoning_outputs.html)
- [Engine Arguments — vLLM](https://docs.vllm.ai/en/latest/serving/engine_args.html)
- [AutoAWQ — vLLM](https://docs.vllm.ai/en/stable/features/quantization/auto_awq.html)
- [[Feature] `reasoning_content` in API for reasoning models like DeepSeek R1 · Issue #12468 · vllm-project/vllm · GitHub](https://github.com/vllm-project/vllm/issues/12468)
- [Deepseek-GenAI/app.py at master · pratik9409/Deepseek-GenAI · GitHub](https://github.com/pratik9409/Deepseek-GenAI/blob/master/app.py)
- [Installation - FlashInfer 0.2.2.post1 documentation](https://docs.flashinfer.ai/installation.html)
- [GitHub - flashinfer-ai/flashinfer: FlashInfer: Kernel Library for LLM Serving](https://github.com/flashinfer-ai/flashinfer)
- [Vllm Flashinfer Overview | Restackio](https://www.restack.io/p/vllm-answer-flashinfer-cat-ai)
- [GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)
- [Models - Hugging Face](https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)
- [unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit · Hugging Face](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit)
- [BitsAndBytes — vLLM](https://docs.vllm.ai/en/latest/features/quantization/bnb.html)
- [Engine Arguments — vLLM](https://docs.vllm.ai/en/v0.5.0.post1/models/engine_args.html)
- [Using VLLM with Langchain for RAG purposes · langchain-ai/langchain · Discussion #22947 · GitHub](https://github.com/langchain-ai/langchain/discussions/22947)
- [BitsAndBytes — vLLM](https://docs.vllm.ai/en/latest/features/quantization/bnb.html)
- [2025 最新 DeepSeek-R1-Distill-Qwen-7B vLLM 部署全攻略：从环境搭建到性能测试(V100-32GB)_人工智能_歌刎-DeepSeek技术社区](https://deepseek.csdn.net/67ab1d6879aaf67875cb9906.html)
- [请问下Qwen2-vl 用vllm部署，如何流式输出，哪有资料可以参考！！ · Issue #278 · QwenLM/Qwen2.5-VL · GitHub](https://github.com/QwenLM/Qwen2.5-VL/issues/278)
- [Can not implement stream in langchain with vllm (qwen2.5) · Issue #29428 · langchain-ai/langchain · GitHub](https://github.com/langchain-ai/langchain/issues/29428)
- [Building RAG Applications with Milvus, Qwen, and vLLM | by Zilliz | Medium](https://medium.com/@zilliz_learn/building-rag-applications-with-milvus-qwen-and-vllm-a063d5ab04db)
- [Building RAG with Milvus, vLLM, and Llama 3.1 | Milvus Documentation](https://milvus.io/docs/milvus_rag_with_vllm.md)
- [Building RAG Applications with Milvus, Qwen, and vLLM | by Zilliz | Medium](https://medium.com/@zilliz_learn/building-rag-applications-with-milvus-qwen-and-vllm-a063d5ab04db)
- [How to using stream output with vllm? · Issue #351 · vllm-project/vllm · GitHub](https://github.com/vllm-project/vllm/issues/351)
- [vllm/vllm/entrypoints/api_server.py at main · vllm-project/vllm · GitHub](https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py#L56-L63)
- [[Usage]: How to use "AsyncLLMEngine" instead of loading the "qwen2-vl-7b" to handle multiple (concurrent) request on single instance. · Issue #13728 · vllm-project/vllm · GitHub](https://github.com/vllm-project/vllm/issues/13728)
- [Working with FAISS Vector DB. Exploring vector storage is pivotal in… | by N CH SHANMUKHA | Medium](https://medium.com/@nchshanmukha999/working-with-faiss-vector-db-86f7154bb937)
- [faiss/tutorial/python/5-Multiple-GPUs.py at main · facebookresearch/faiss · GitHub](https://github.com/facebookresearch/faiss/blob/main/tutorial/python/5-Multiple-GPUs.py)
- [Multi GPU support · Issue #3486 · langchain-ai/langchain · GitHub](https://github.com/langchain-ai/langchain/issues/3486)
- [Faiss on the GPU · facebookresearch/faiss Wiki · GitHub](https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU)
- [Threads and asynchronous calls · facebookresearch/faiss Wiki · GitHub](https://github.com/facebookresearch/faiss/wiki/Threads-and-asynchronous-calls)
- [Running on GPUs · facebookresearch/faiss Wiki · GitHub](https://github.com/facebookresearch/faiss/wiki/Running-on-GPUs)
- [GitHub - facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors.](https://github.com/facebookresearch/faiss)
- [Building FAISS GPU for my laptop 1080 on Ubuntu WSL · GitHub](https://gist.github.com/jlmelville/9b4f0d91ede13bff18d26759140709f9)
- [Faiss | 🦜️🔗 LangChain](https://python.langchain.com/docs/integrations/vectorstores/faiss/#saving-and-loading)
- [FAISS — 🦜🔗 LangChain  documentation](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html)
- [FindPython — CMake 4.0.0-rc4 Documentation](https://cmake.org/cmake/help/latest/module/FindPython.html)
- [faiss/INSTALL.md at main · facebookresearch/faiss · GitHub](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md)
- [CUDA GPUs - Compute Capability | NVIDIA Developer](https://developer.nvidia.com/cuda-gpus)
- [介绍 - OpenAI（ChatGPT）](https://openai.apifox.cn/)
