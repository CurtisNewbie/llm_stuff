### LLM

- [GitHub - mlrun/demo-llm-tuning: Demo MLRun project for LLM Tuning and serving pipelines](https://github.com/mlrun/demo-llm-tuning)
- [æ¨¡å‹åˆ—è¡¨_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/getting-started/models?spm=a2c4g.11186623.help-menu-2400256.d_0_2.4d7220efgSTxpX&scm=20140722.H_2840914._.OR_help-T_cn~zh-V_1#ced16cb6cdfsy)
- [åœ¨æ§åˆ¶å°ä½¿ç”¨æ¨¡å‹è°ƒä¼˜_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/user-guide/using-fine-tuning-on-console?spm=a2c4g.11186623.help-menu-2400256.d_1_3_0.42a635254ifadK&scm=20140722.H_2587462._.OR_help-T_cn~zh-V_1#f161782782o0n)
- [æ¨¡å‹æ¨ç†ã€æ¨¡å‹è®­ç»ƒã€æ¨¡å‹éƒ¨ç½²è´¹ç”¨è¯´æ˜_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/billing-for-model-studio?spm=a2c4g.11186623.0.0.3cba5991yReIGZ#7a3514acc706i)
- [Code completion | IntelliJÂ IDEA Documentation](https://www.jetbrains.com/help/idea/auto-completing-code.html#basic_completion)
- [GitHub - JetBrains/intellij-sdk-docs: IntelliJ SDK Platform Documentation](https://github.com/JetBrains/intellij-sdk-docs)
- [GitHub - continuedev/continue: Continue is the leading open-source AI code assistant. You can connect any models and any context to build custom autocomplete and chat experiences inside VS Code and JetBrains](https://github.com/continuedev/continue)
- [copilot-explorer | Hacky repo to see what the Copilot extension sends to the server](https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html)
- [How it works | Continue](https://docs.continue.dev/autocomplete/how-it-works)
- [GitHub - JetBrains/intellij-community: IntelliJ IDEA Community Edition & IntelliJ Platform](https://github.com/jetbrains/intellij-community)
- [mlrun - Using LLMs to process unstructured data](https://docs.mlrun.org/en/v1.6.4/genai/data-mgmt/unstructured-data.html#turning-unstructured-data-into-structured-data)
- [aliyun - RAGæ•ˆæœä¼˜åŒ–_å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼(Model Studio)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/model-studio/use-cases/rag-optimization?spm=a2c4g.11186623.help-menu-2400256.d_2_12.4f01716edKjj8o&scm=20140722.H_2859102._.OR_help-T_cn~zh-V_1)
- [GitHub - jlonge4/local_llama: This repo is to showcase how you can run a model locally and offline, free of OpenAI dependencies.](https://github.com/jlonge4/local_llama)
- [Ollama](https://ollama.com/)
- [GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.](https://github.com/ollama/ollama)
- [GitHub - Mintplex-Labs/anything-llm: The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, and more.](https://github.com/Mintplex-Labs/anything-llm)
- [localai.io - Overview | LocalAI documentation](https://localai.io/)
- [ollama - deepseek-r1](https://ollama.com/library/deepseek-r1)
- [é˜¿é‡Œäº‘ - RAG çŸ¥è¯†åº“](https://help.aliyun.com/zh/model-studio/user-guide/rag-knowledge-base?spm=a2c4g.11186623.help-menu-2400256.d_1_8_2_0_0.4d666373lBKTyJ)
- [Using LLMs to process unstructured data](https://docs.mlrun.org/en/v1.6.4/genai/data-mgmt/unstructured-data.html#turning-unstructured-data-into-structured-data)
- [MLRun Â· GitHub](https://github.com/mlrun)
- [GitHub - mlrun/demo-llm-tuning: Demo MLRun project for LLM Tuning and serving pipelines](https://github.com/mlrun/demo-llm-tuning)
- [GitHub - facebookincubator/Glean: System for collecting, deriving and working with facts about source code.](https://github.com/facebookincubator/Glean)
- [Introduction | Glean](https://glean.software/docs/introduction/)
- [GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.](https://github.com/ollama/ollama?tab=readme-ov-file)
- [Fine-Tuning With Ollama Techniques | Restackio](https://www.restack.io/p/fine-tuning-answer-ollama-techniques-cat-ai)
- [deepseek-r1:1.5b](https://ollama.com/library/deepseek-r1:1.5b)
https://dev.to/mohsin_rashid_13537f11a91/rag-with-ollama-1049, HTTP Error 403:
- [ollama/docs/modelfile.md at main Â· ollama/ollama Â· GitHub](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#build-from-existing-model)
- [GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)
- [GitHub - langgenius/dify: Dify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.](https://github.com/langgenius/dify)
- [How to use JSON references ($refs)](https://redocly.com/learn/openapi/ref-guide)
- [Qwen/Qwen2.5-32B Â· Hugging Face](https://huggingface.co/Qwen/Qwen2.5-32B)
- [GitHub - sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models.](https://github.com/sgl-project/sglang)
- [GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)
- [GitHub - facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors.](https://github.com/facebookresearch/faiss?tab=readme-ov-file)
https://www.reddit.com/r/LocalLLaMA/comments/1fm59kg/how_do_you_actually_finetune_a_llm_on_your_own/, HTTP Error 403:
- [GitHub - Lightning-AI/litgpt: 20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.](https://github.com/Lightning-AI/litgpt)
- [GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1 & Reasoning LLMs 2x faster with 70% less memory! ğŸ¦¥](https://github.com/unslothai/unsloth?tab=readme-ov-file)
https://unsloth.ai/blog/deepseek-r1, HTTP Error 403:
https://www.datacamp.com/tutorial/fine-tuning-large-language-models, HTTP Error 403:
- [Implementing RAG in Refact.ai AI Coding Assistant | by Refact.ai | Medium](https://medium.com/@refact_ai/implementing-rag-in-refact-ai-ai-coding-assistant-79e98b359a83)
- [generative-ai/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb at main Â· GoogleCloudPlatform/generative-ai Â· GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/code_rag.ipynb)
- [RAG For a Codebase with 10k Repos](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/)
- [Chunking 2M+ files a day for Code Search using Syntax Trees](https://docs.sweep.dev/blogs/chunking-2m-files)
- [sweep/sweepai/utils/utils.py at b267b613d4c706eaf959fe6789f11e9a856521d1 Â· sweepai/sweep Â· GitHub](https://github.com/sweepai/sweep/blob/b267b613d4c706eaf959fe6789f11e9a856521d1/sweepai/utils/utils.py#L48-L126)
- [GitHub - run-llama/llama_index: LlamaIndex is the leading framework for building LLM-powered agents over your data.](https://github.com/run-llama/llama_index)
- [LlamaIndex - LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [GGUF](https://huggingface.co/docs/hub/en/gguf)
- [Continue - Codestral, Claude, and more - Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
- [Explore LLM using LangChain and HuggingFace | kekw](https://curtisnewbie.github.io/learning/2024/06/24/explore-llm-using-langchain-and-huggingface.html)
- [GitHub - qiyu-kefu/message_interface: ä¸ƒé±¼æ¶ˆæ¯æ¥å£ä»¥åŠç®€å•å¾®ä¿¡å…¬ä¼—å·å¼€å‘æ¨¡å¼ç¤ºä¾‹demo](https://github.com/qiyu-kefu/message_interface)
- [deepseek-r1 32Béœ€è¦ä»€ä¹ˆé…ç½® - CSDNæ–‡åº“](https://wenku.csdn.net/answer/7nz5wit3zf)
- [å¦‚ä½•æœ¬åœ°éƒ¨ç½²DeepSeek_äº‘æœåŠ¡å™¨ ECS(ECS)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ](https://help.aliyun.com/zh/ecs/use-cases/deploy-the-deepseek-r1-distill-model-on-a-gpu-accelerated-instance)
- [NVIDIAçš„GA107ã€A10ã€V100ã€H100ã€H200å’Œæ¶ˆè´¹çº§æ˜¾å¡çš„åŒºåˆ«è‹±ä¼Ÿè¾¾çš„GA107ã€A10å’ŒV100æ˜¯ä¸ - æ˜é‡‘](https://juejin.cn/post/7371273982200004635)
- [NVIDIA A10 ä¸ A100 GPU å¯¹æ¯”åˆ†æï¼šç”¨äºLLM å’ŒStable Diffusionæ¨ç†](https://www.jaeaiot.com/news/detail/295.html)
- [Using VLLM with Langchain for RAG purposes Â· langchain-ai/langchain Â· Discussion #22947 Â· GitHub](https://github.com/langchain-ai/langchain/discussions/22947)
- [FAISS Vector Database for Production LLM Applications | by Wamiq Raza | Pythonâ€™s Gurus | Medium](https://medium.com/pythons-gurus/faiss-vector-database-for-production-llm-applications-90273c78fcbf)
- [Faiss | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/integrations/vectorstores/faiss/)
- [Welcome to Faiss Documentation â€” Faiss  documentation](https://faiss.ai/index.html)
- [faiss/INSTALL.md at main Â· facebookresearch/faiss Â· GitHub](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md)
- [GitHub - deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file)
- [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B Â· Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B#usage-recommendations)
- [deepseek-ai (DeepSeek)](https://huggingface.co/deepseek-ai)
- [Models - Hugging Face](https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)
- [vLLM | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/integrations/llms/vllm/)
- [llmc/docs/zh_cn/source/backend/vllm.md at main Â· ModelTC/llmc Â· GitHub](https://github.com/ModelTC/llmc/blob/main/docs/zh_cn/source/backend/vllm.md)
- [How to use Streaming in LangChain and Streamlit Â· Alejandro AO](https://alejandro-ao.com/how-to-use-streaming-in-langchain-and-streamlit/)
- [How to stream runnables | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/how_to/streaming/)
- [Help: åœ¨ollamaä¸Šè¿è¡Œæ—¶, æ€ä¹ˆæ‰èƒ½è®©å®ƒä¸è¿”å›<think>æ ‡ç­¾ ?  When running on ollama, how can I make it not return the < think > tag? Â· Issue #23 Â· deepseek-ai/DeepSeek-R1 Â· GitHub](https://github.com/deepseek-ai/DeepSeek-R1/issues/23)
- [Reasoning Outputs â€” vLLM](https://docs.vllm.ai/en/latest/features/reasoning_outputs.html)
- [Engine Arguments â€” vLLM](https://docs.vllm.ai/en/latest/serving/engine_args.html)
- [AutoAWQ â€” vLLM](https://docs.vllm.ai/en/stable/features/quantization/auto_awq.html)
- [[Feature] `reasoning_content` in API for reasoning models like DeepSeek R1 Â· Issue #12468 Â· vllm-project/vllm Â· GitHub](https://github.com/vllm-project/vllm/issues/12468)
- [Deepseek-GenAI/app.py at master Â· pratik9409/Deepseek-GenAI Â· GitHub](https://github.com/pratik9409/Deepseek-GenAI/blob/master/app.py)
- [Installation - FlashInfer 0.2.2.post1 documentation](https://docs.flashinfer.ai/installation.html)
- [GitHub - flashinfer-ai/flashinfer: FlashInfer: Kernel Library for LLM Serving](https://github.com/flashinfer-ai/flashinfer)
- [Vllm Flashinfer Overview | Restackio](https://www.restack.io/p/vllm-answer-flashinfer-cat-ai)
- [GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)
- [Models - Hugging Face](https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)
- [unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit Â· Hugging Face](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit)
- [BitsAndBytes â€” vLLM](https://docs.vllm.ai/en/latest/features/quantization/bnb.html)
- [Engine Arguments â€” vLLM](https://docs.vllm.ai/en/v0.5.0.post1/models/engine_args.html)
- [Using VLLM with Langchain for RAG purposes Â· langchain-ai/langchain Â· Discussion #22947 Â· GitHub](https://github.com/langchain-ai/langchain/discussions/22947)
- [BitsAndBytes â€” vLLM](https://docs.vllm.ai/en/latest/features/quantization/bnb.html)
- [2025 æœ€æ–° DeepSeek-R1-Distill-Qwen-7B vLLM éƒ¨ç½²å…¨æ”»ç•¥ï¼šä»ç¯å¢ƒæ­å»ºåˆ°æ€§èƒ½æµ‹è¯•(V100-32GB)_äººå·¥æ™ºèƒ½_æ­Œåˆ-DeepSeekæŠ€æœ¯ç¤¾åŒº](https://deepseek.csdn.net/67ab1d6879aaf67875cb9906.html)
- [è¯·é—®ä¸‹Qwen2-vl ç”¨vllméƒ¨ç½²ï¼Œå¦‚ä½•æµå¼è¾“å‡ºï¼Œå“ªæœ‰èµ„æ–™å¯ä»¥å‚è€ƒï¼ï¼ Â· Issue #278 Â· QwenLM/Qwen2.5-VL Â· GitHub](https://github.com/QwenLM/Qwen2.5-VL/issues/278)
- [Can not implement stream in langchain with vllm (qwen2.5) Â· Issue #29428 Â· langchain-ai/langchain Â· GitHub](https://github.com/langchain-ai/langchain/issues/29428)
- [Building RAG Applications with Milvus, Qwen, and vLLM | by Zilliz | Medium](https://medium.com/@zilliz_learn/building-rag-applications-with-milvus-qwen-and-vllm-a063d5ab04db)
- [Building RAG with Milvus, vLLM, and Llama 3.1 | Milvus Documentation](https://milvus.io/docs/milvus_rag_with_vllm.md)
- [Building RAG Applications with Milvus, Qwen, and vLLM | by Zilliz | Medium](https://medium.com/@zilliz_learn/building-rag-applications-with-milvus-qwen-and-vllm-a063d5ab04db)
- [How to using stream output with vllm? Â· Issue #351 Â· vllm-project/vllm Â· GitHub](https://github.com/vllm-project/vllm/issues/351)
- [vllm/vllm/entrypoints/api_server.py at main Â· vllm-project/vllm Â· GitHub](https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py#L56-L63)
- [[Usage]: How to use "AsyncLLMEngine" instead of loading the "qwen2-vl-7b" to handle multiple (concurrent) request on single instance. Â· Issue #13728 Â· vllm-project/vllm Â· GitHub](https://github.com/vllm-project/vllm/issues/13728)
- [Working with FAISS Vector DB. Exploring vector storage is pivotal inâ€¦ | by N CH SHANMUKHA | Medium](https://medium.com/@nchshanmukha999/working-with-faiss-vector-db-86f7154bb937)
- [faiss/tutorial/python/5-Multiple-GPUs.py at main Â· facebookresearch/faiss Â· GitHub](https://github.com/facebookresearch/faiss/blob/main/tutorial/python/5-Multiple-GPUs.py)
- [Multi GPU support Â· Issue #3486 Â· langchain-ai/langchain Â· GitHub](https://github.com/langchain-ai/langchain/issues/3486)
- [Faiss on the GPU Â· facebookresearch/faiss Wiki Â· GitHub](https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU)
- [Threads and asynchronous calls Â· facebookresearch/faiss Wiki Â· GitHub](https://github.com/facebookresearch/faiss/wiki/Threads-and-asynchronous-calls)
- [Running on GPUs Â· facebookresearch/faiss Wiki Â· GitHub](https://github.com/facebookresearch/faiss/wiki/Running-on-GPUs)
- [GitHub - facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors.](https://github.com/facebookresearch/faiss)
- [Building FAISS GPU for my laptop 1080 on Ubuntu WSL Â· GitHub](https://gist.github.com/jlmelville/9b4f0d91ede13bff18d26759140709f9)
- [Faiss | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/integrations/vectorstores/faiss/#saving-and-loading)
- [FAISS â€” ğŸ¦œğŸ”— LangChain  documentation](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html)
- [FindPython â€” CMake 4.0.0-rc4 Documentation](https://cmake.org/cmake/help/latest/module/FindPython.html)
- [faiss/INSTALL.md at main Â· facebookresearch/faiss Â· GitHub](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md)
- [CUDA GPUs - Compute Capability | NVIDIA Developer](https://developer.nvidia.com/cuda-gpus)
- [ä»‹ç» - OpenAIï¼ˆChatGPTï¼‰](https://openai.apifox.cn/)
